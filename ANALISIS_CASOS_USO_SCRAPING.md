# AnÃ¡lisis: Casos de Uso de Scraping

## ğŸ“‹ Tres Casos de Uso Identificados

### 1. ğŸ”„ Scraping Continuo 24/7 (Monitoreo de Movimientos Nuevos)
**Objetivo**: Recorrer causas periÃ³dicamente buscando movimientos nuevos, poblando SQL todo el dÃ­a

### 2. ğŸ¯ Scraping por Endpoint (Gatillado por Nuevo Ingreso a BD)
**Objetivo**: Endpoint HTTP que se gatilla cuando hay un nuevo ingreso a la base de datos

### 3. ğŸ“¦ Scraping Masivo (Una Vez, Poblar Toda la BD)
**Objetivo**: Recorrer todas las causas una vez para poblar toda la base de datos (ejecuciÃ³n Ãºnica)

---

## âœ… AnÃ¡lisis por Caso

### 1ï¸âƒ£ Scraping Continuo 24/7 (Monitoreo de Movimientos Nuevos)

#### âŒ **NO estÃ¡ completamente cubierto**

**Archivos relevantes refactorizados:**
- âŒ `worker_cola_scraping.js` - Procesa cola, pero no estÃ¡ diseÃ±ado para polling continuo
- âŒ `scraping_masivo.js` - Recorre todas las causas, pero no estÃ¡ diseÃ±ado para ejecuciÃ³n continua

**Archivos existentes (no refactorizados):**
- âœ… `src/api/listener.js` - Detecta nuevos registros en BD y los agrega a cola
- âš ï¸ `worker_cola_scraping.js` - Procesa cola, pero necesita adaptaciÃ³n

**Lo que falta:**
1. **Worker de monitoreo continuo** que:
   - Recorra todas las causas activas periÃ³dicamente (ej: cada 1 hora)
   - Compare movimientos actuales vs movimientos en BD
   - Detecte movimientos nuevos
   - Solo procese causas con movimientos nuevos
   - Se ejecute 24/7 sin parar

**RecomendaciÃ³n:**
- Crear nuevo: `src/worker-monitoreo-continuo.js`
- Usar `processCausa` para scraping
- Comparar movimientos con BD para detectar nuevos
- Configurar intervalo de polling (ej: `--interval 3600000` = 1 hora)

---

### 2ï¸âƒ£ Scraping por Endpoint (Gatillado por Nuevo Ingreso a BD)

#### âœ… **CUBIERTO, pero necesita verificaciÃ³n**

**Archivos relevantes refactorizados:**
- âœ… `api/scraper-service.js` - Ya usa `processCausa`, exporta `ejecutarScraping()`
- âœ… `worker_cola_scraping.js` - Procesa cola usando `processCausa`

**Archivos existentes (no refactorizados):**
- âœ… `src/api/listener.js` - Detecta nuevos registros en BD y los agrega a cola
- âœ… `src/api/mvp-api.js` - Tiene ruta `POST /api/mvp/scraping/ejecutar` que usa `executeScraping`
- âœ… `src/api/scraping-api.js` - Tiene rutas que usan `ejecutarScraping`

**Arquitectura actual:**
```
OpciÃ³n A (Cola):
Nuevo registro en BD
    â†“
listener.js detecta
    â†“
Agrega a pjud_cola_scraping
    â†“
worker_cola_scraping.js procesa (usa processCausa)

OpciÃ³n B (Endpoint directo):
POST /api/mvp/scraping/ejecutar
    â†“
ejecutarScraping() (usa processCausa)
    â†“
Retorna resultado HTTP
```

**Lo que funciona:**
- âœ… `listener.js` detecta nuevos registros
- âœ… `worker_cola_scraping.js` procesa la cola usando `processCausa`
- âœ… `api/mvp-api.js` tiene ruta `POST /api/mvp/scraping/ejecutar`
- âœ… `api/scraping-api.js` tiene rutas de scraping

**âš ï¸ VerificaciÃ³n necesaria:**
1. Verificar que `mvp-api.js` importa correctamente (`ejecutarScraping` vs `executeScraping`)
2. Verificar que las rutas funcionan correctamente
3. Opcional: Agregar ruta mÃ¡s simple `POST /api/scrape` que reciba RIT directamente

**RecomendaciÃ³n:**
- âœ… Ya estÃ¡ implementado
- Verificar/corregir imports en `mvp-api.js` si es necesario
- Probar endpoints existentes

---

### 3ï¸âƒ£ Scraping Masivo (Una Vez, Poblar Toda la BD)

#### âœ… **COMPLETAMENTE cubierto**

**Archivos relevantes refactorizados:**
- âœ… `scraping_masivo.js` - Recorre todas las causas desde CSV, usa `processCausa`
- âœ… `process-causas.js` - Tiene `processMultipleCausas()` que procesa desde CSV

**Funcionalidad:**
- âœ… Lee CSV de causas (`causa.csv`)
- âœ… Filtra causas vÃ¡lidas (con tribunal)
- âœ… Procesa cada causa usando `processCausa`
- âœ… Guarda checkpoint para poder continuar si se interrumpe
- âœ… Pausa entre causas para respetar lÃ­mites

**Uso:**
```bash
npm run scrape:masivo
# o
node src/scraping_masivo.js
```

**Estado:**
- âœ… **LISTO PARA USAR**
- âœ… Ya usa `processCausa` (refactorizado)
- âœ… Puede poblar toda la BD en una ejecuciÃ³n

---

## ğŸ“Š Resumen de Cobertura

| Caso de Uso | Estado | Archivos Relevantes | AcciÃ³n Requerida |
|------------|--------|---------------------|------------------|
| **1. Continuo 24/7** | âŒ No cubierto | `worker_cola_scraping.js` (parcial) | Crear `worker-monitoreo-continuo.js` |
| **2. Por Endpoint** | âœ… Parcialmente | `api/scraper-service.js` âœ… | Agregar ruta HTTP directa |
| **3. Masivo (una vez)** | âœ… Completo | `scraping_masivo.js` âœ… | Nada, listo para usar |

---

## ğŸ› ï¸ Recomendaciones de ImplementaciÃ³n

### Para Caso 1 (Continuo 24/7):

**Crear:** `src/worker-monitoreo-continuo.js`

```javascript
// PseudocÃ³digo
async function workerMonitoreoContinuo() {
  while (true) {
    // 1. Obtener todas las causas activas de BD
    const causasActivas = await obtenerCausasActivas();
    
    for (const causa of causasActivas) {
      // 2. Scraping usando processCausa
      const resultado = await processCausa(page, context, causa, outputDir);
      
      // 3. Comparar movimientos con BD
      const movimientosNuevos = await detectarMovimientosNuevos(causa.rit, resultado);
      
      // 4. Solo actualizar si hay movimientos nuevos
      if (movimientosNuevos.length > 0) {
        await actualizarMovimientosEnBD(causa.rit, movimientosNuevos);
      }
    }
    
    // 5. Esperar intervalo (ej: 1 hora)
    await sleep(INTERVAL_MS);
  }
}
```

**CaracterÃ­sticas:**
- Usa `processCausa` (ya refactorizado)
- Compara movimientos para detectar solo nuevos
- Se ejecuta 24/7
- Configurable intervalo

### Para Caso 2 (Por Endpoint):

**OpciÃ³n A: Usar `scraper-service.js` directamente**
```javascript
// En api/server.js o api/mvp-api.js
app.post('/api/scrape', async (req, res) => {
  const { ejecutarScraping } = require('./scraper-service');
  const resultado = await ejecutarScraping(req.body);
  res.json(resultado);
});
```

**OpciÃ³n B: Integrar con listener**
```javascript
// Modificar listener.js para llamar endpoint directamente
// O mantener cola pero con procesamiento inmediato
```

### Para Caso 3 (Masivo):

**âœ… Ya estÃ¡ listo:**
```bash
npm run scrape:masivo
```

---

## ğŸ¯ ConclusiÃ³n

### Archivos Refactorizados que SÃ se UsarÃ¡n:

1. âœ… **`process-causa.js`** - Motor Ãºnico usado por TODOS
2. âœ… **`scraping_masivo.js`** - Caso 3 (masivo una vez)
3. âœ… **`api/scraper-service.js`** - Caso 2 (endpoint)
4. âš ï¸ **`worker_cola_scraping.js`** - Caso 2 (parcial, procesa cola)

### Archivos Refactorizados que NO se UsarÃ¡n:

1. âŒ `index.js` - Solo para testing manual
2. âŒ `index-sin-pausa.js` - Solo para testing manual
3. âŒ `scraper_batch.js` - Ya no se usa (reemplazado por `scraping_masivo.js`)
4. âŒ `processRit.js` - Solo shim de compatibilidad

### Lo que Falta Crear:

1. ğŸ”¨ **`worker-monitoreo-continuo.js`** - Para Caso 1 (24/7)
2. ğŸ”¨ **Ruta HTTP en API** - Para Caso 2 (endpoint directo)

---

## âœ… Plan de AcciÃ³n

1. **Caso 3 (Masivo)**: âœ… Ya estÃ¡ listo, usar `scraping_masivo.js`

2. **Caso 2 (Endpoint)**: 
   - Agregar ruta HTTP que use `scraper-service.js`
   - O mejorar integraciÃ³n `listener.js` â†’ endpoint

3. **Caso 1 (Continuo 24/7)**:
   - Crear `worker-monitoreo-continuo.js`
   - Implementar lÃ³gica de detecciÃ³n de movimientos nuevos
   - Configurar para ejecuciÃ³n 24/7
