# üìã Casos de Uso de Scraping - Documentaci√≥n Oficial

Este documento describe los **3 casos de uso principales** del sistema de scraping PJUD y los archivos correspondientes.

---

## üéØ Los 3 Casos de Uso

### 1Ô∏è‚É£ Scraping Continuo 24/7 (Monitoreo de Movimientos Nuevos)

**Objetivo**: Recorrer causas activas peri√≥dicamente buscando movimientos nuevos, poblando SQL todo el d√≠a de forma continua.

**Archivo**: `src/worker-monitoreo-continuo.js`

**Caracter√≠sticas**:
- ‚úÖ Se ejecuta 24/7 sin parar
- ‚úÖ Recorre todas las causas activas de la BD peri√≥dicamente
- ‚úÖ Usa `processCausa` (motor √∫nico) para scraping
- ‚úÖ Compara movimientos con BD para detectar solo movimientos nuevos
- ‚úÖ Actualiza BD solo cuando hay movimientos nuevos
- ‚úÖ Configurable intervalo de polling (default: 1 hora)

**Uso**:
```bash
# Ejecutar con intervalo por defecto (1 hora)
npm run scrape:monitoreo

# Ejecutar con intervalo personalizado (30 minutos)
node src/worker-monitoreo-continuo.js --interval 1800000

# Ejecutar una vez y terminar (para testing)
node src/worker-monitoreo-continuo.js --once
```

**Flujo**:
```
1. Obtener causas activas de BD
   ‚Üì
2. Para cada causa:
   - Scraping usando processCausa
   - Comparar movimientos con BD
   - Detectar movimientos nuevos
   - Actualizar BD solo con nuevos
   ‚Üì
3. Esperar intervalo (ej: 1 hora)
   ‚Üì
4. Repetir (24/7)
```

**Configuraci√≥n**:
- Intervalo por defecto: 1 hora (3600000 ms)
- Pausa entre causas: 2 segundos
- Tabla de causas: `causa`
- Tabla de movimientos: `pjud_movimientos_intermedia`

---

### 2Ô∏è‚É£ Scraping por Endpoint (Gatillado por Nuevo Ingreso a BD)

**Objetivo**: Endpoint HTTP que se gatilla cuando hay un nuevo ingreso a la base de datos, ejecutando scraping inmediatamente.

**Archivo**: `src/api/scraper-service.js`

**Caracter√≠sticas**:
- ‚úÖ Endpoint HTTP (`POST /api/scraping/ejecutar`)
- ‚úÖ Usa `processCausa` (motor √∫nico) para scraping
- ‚úÖ Retorna resultado inmediato v√≠a HTTP
- ‚úÖ Puede ser llamado desde listener o directamente
- ‚úÖ Guarda resultados en BD y archivos

**Uso**:
```bash
# Iniciar servidor API
npm run api:start

# Llamar endpoint desde otro sistema
curl -X POST http://localhost:3000/api/scraping/ejecutar \
  -H "Content-Type: application/json" \
  -d '{
    "rit": "C-3030-2017",
    "competencia": "3",
    "corte": "90",
    "tribunal": "61",
    "tipoCausa": "C"
  }'
```

**Endpoints disponibles**:
- `POST /api/scraping/ejecutar` - Ejecuta scraping inmediatamente
- `POST /api/mvp/scraping/ejecutar` - Ejecuta scraping (MVP, requiere auth)
- `GET /api/scraping/resultado/:rit` - Obtiene resultado por RIT

**Integraci√≥n con Listener**:
```
Nuevo registro en BD
    ‚Üì
src/api/listener.js detecta
    ‚Üì
Puede llamar directamente al endpoint
    o
Agregar a cola ‚Üí worker_cola_scraping.js procesa
```

**Flujo**:
```
1. Request HTTP con RIT y configuraci√≥n
   ‚Üì
2. Validar datos
   ‚Üì
3. Scraping usando processCausa
   ‚Üì
4. Guardar en BD y archivos
   ‚Üì
5. Retornar resultado HTTP
```

---

### 3Ô∏è‚É£ Scraping Masivo (Una Vez, Poblar Toda la BD)

**Objetivo**: Recorrer todas las causas una vez para poblar toda la base de datos. Ejecuci√≥n √∫nica, no continua.

**Archivo**: `src/scraping_masivo.js`

**Caracter√≠sticas**:
- ‚úÖ Recorre todas las causas desde CSV (`causa.csv`)
- ‚úÖ Usa `processCausa` (motor √∫nico) para scraping
- ‚úÖ Guarda checkpoint para poder continuar si se interrumpe
- ‚úÖ Pausa entre causas para respetar l√≠mites de PJUD
- ‚úÖ Ejecuci√≥n √∫nica, no continua

**Uso**:
```bash
# Ejecutar scraping masivo
npm run scrape:masivo

# O directamente
node src/scraping_masivo.js
```

**Flujo**:
```
1. Leer CSV de causas (causa.csv)
   ‚Üì
2. Filtrar causas v√°lidas (con tribunal)
   ‚Üì
3. Para cada causa:
   - Scraping usando processCausa
   - Guardar resultados
   - Guardar checkpoint
   - Pausa entre causas
   ‚Üì
4. Continuar desde checkpoint si se interrumpe
```

**Checkpoint**:
- Archivo: `src/rit_state.json`
- Guarda √∫ltimo RIT procesado
- Permite reanudar si se interrumpe

**Configuraci√≥n**:
- CSV de entrada: `causa.csv`
- Directorio de salida: `src/outputs/`
- Pausa entre causas: 5 segundos (respetando l√≠mite de 200 b√∫squedas)

---

## üîß Motor √önico: `processCausa`

**Todos los casos de uso usan el mismo motor**:

**Archivo**: `src/process-causas.js`

**Funci√≥n principal**: `processCausa(page, context, config, outputDir)`

**Ventajas**:
- ‚úÖ Consistencia: Todos los flujos usan la misma l√≥gica
- ‚úÖ Mantenibilidad: Un solo lugar para arreglar bugs
- ‚úÖ Testing: M√°s f√°cil testear el motor una vez
- ‚úÖ Documentaci√≥n: Est√°ndar claro de c√≥mo hacer scraping

**Ver documentaci√≥n completa**: `docs/scraping-standard.md`

---

## üìä Resumen de Archivos

| Caso de Uso | Archivo Principal | Uso | Estado |
|------------|-------------------|-----|--------|
| **1. Continuo 24/7** | `src/worker-monitoreo-continuo.js` | `npm run scrape:monitoreo` | ‚úÖ Listo |
| **2. Por Endpoint** | `src/api/scraper-service.js` | `POST /api/scraping/ejecutar` | ‚úÖ Listo |
| **3. Masivo (una vez)** | `src/scraping_masivo.js` | `npm run scrape:masivo` | ‚úÖ Listo |
| **Motor √∫nico** | `src/process-causas.js` | Usado por todos | ‚úÖ Motor |

---

## üöÄ Inicio R√°pido

### Para monitoreo continuo 24/7:
```bash
npm run scrape:monitoreo
```

### Para scraping por endpoint:
```bash
# Iniciar servidor
npm run api:start

# Llamar endpoint (desde otro sistema o curl)
curl -X POST http://localhost:3000/api/scraping/ejecutar \
  -H "Content-Type: application/json" \
  -d '{"rit": "C-3030-2017", "competencia": "3", "corte": "90", "tribunal": "61", "tipoCausa": "C"}'
```

### Para scraping masivo (una vez):
```bash
npm run scrape:masivo
```

---

## üìù Notas Importantes

1. **Todos los casos usan `processCausa`**: Esto garantiza consistencia y facilita mantenimiento.

2. **Configuraci√≥n de BD**: Todos requieren variables de entorno:
   - `DB_HOST`
   - `DB_PORT`
   - `DB_NAME`
   - `DB_USER`
   - `DB_PASSWORD`

3. **L√≠mites de PJUD**: El sistema respeta l√≠mites de b√∫squeda (200 por d√≠a) con pausas entre causas.

4. **Checkpoints**: El scraping masivo guarda checkpoints para poder continuar si se interrumpe.

5. **Detecci√≥n de movimientos nuevos**: El monitoreo continuo compara movimientos para detectar solo nuevos.

---

## üîç Verificaci√≥n

Para verificar que solo estos 3 archivos hacen scraping (adem√°s del motor):

```bash
# Buscar violaciones (deber√≠a mostrar solo helpers del motor + tests/debug)
grep -r "fillForm|openDetalle|extractTable" src --include="*.js" | \
  grep -v "process-causas.js" | \
  grep -v "form.js" | \
  grep -v "table.js" | \
  grep -v "browser.js" | \
  grep -v "navigation.js" | \
  grep -v "test/" | \
  grep -v "debug"
```

Si hay resultados, son violaciones que deben ser adapters que llamen a `processCausa`.

---

## üìö Documentaci√≥n Relacionada

- `docs/scraping-standard.md` - Est√°ndar de scraping (motor √∫nico)
- `ANALISIS_CASOS_USO_SCRAPING.md` - An√°lisis detallado de casos de uso
- `INFORME_SCRAPING_STANDARD.md` - Informe de implementaci√≥n del est√°ndar
